# -*- coding: utf-8 -*-
"""Image Classification-notebook.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uAQzmjcot81POLDmAR0UWt9i5bA6rcjm

# Proyek Klasifikasi Gambar: Satellite Image Classification
- **Nama:** Geralda Livia Nugraha
- **Email:** geraldaliviaa93@gmail.com || mc299d5x1168@student.devacademy.id
- **ID Dicoding:** alddar (MC299D5X1168)

Archieve : https://drive.google.com/drive/folders/1T4GI5UtHUBpL9ogif97WmlBHLoJr7mRw?usp=sharing

## Import Semua Packages/Library yang Digunakan
"""

# Intall Depedencies
!pip install tensorflowjs
!pip freeze > requirements.txt

# Library
import os
import shutil
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
import random
from pathlib import Path

# Load Data dan Akses ke Drive
from google.colab import drive

# Processing Data
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.preprocessing import image

# Build Model
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Training Model
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau

# Save Model
import tensorflow as tf
import tensorflowjs as tfjs

# Inference
from PIL import Image

# Untuk requirement.txt
from google.colab import files
files.download('requirements.txt')

"""## Data Preparation

### Data Loading
"""

# Mounting GDrive
drive.mount('/content/drive')

# Load dataset .zip
!unzip "/content/drive/MyDrive/Submission/Deep Learning/Image Classification/Dataset/Satellite.zip" -d "/content/satellite_dataset"

"""### Data Exploratory"""

# Mengenali Dataset
path = "/content/satellite_dataset/data"

classes = sorted([d for d in os.listdir(path) if os.path.isdir(os.path.join(path, d))])
print("Classes:", classes)

class_counts = {}
for cls in classes:
    cls_path = os.path.join(path, cls)
    images = [f for f in os.listdir(cls_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]
    class_counts[cls] = len(images)

data = pd.DataFrame(list(class_counts.items()), columns=['Class', 'Count'])
print(data)

# Menampilkan Plot tiap Class
plt.figure(figsize=(10, 6))
sns.barplot(x='Class', y='Count', data=data, palette='pastel')
plt.title('Class Distribution')
plt.xlabel('Image of Satellite')
plt.ylabel('Number of Images')
plt.xticks(rotation=45)
plt.show()

# Menampilkan visualisasi beberapa gambar dari tiap kelas
fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(15, 10))
axes = axes.flatten()

for i, cls in enumerate(classes):
    if i >= 12:
        break
    img_path = os.path.join(path, cls, os.listdir(os.path.join(path, cls))[0])
    img = plt.imread(img_path)
    axes[i].imshow(img)
    axes[i].set_title(cls)
    axes[i].axis('off')

plt.tight_layout()
plt.show()

"""### Data Preprocessing

#### Split Dataset
"""

# Path dataset yang digunakan
path = "/content/satellite_dataset/data"
new_path = "/content/satellite_dataset_split" # buat output

# Rasio pembagian data
train_ratio = 0.7  # 70% training
val_ratio = 0.15   # 15% validation
test_ratio = 0.15  # 15% testing

# Daftar kelas
classes = ['cloudy', 'desert', 'green_area', 'water']

# Membuat folder untuk output berdasarkan train, val, test
for split in ['train', 'val', 'test']:
    for cls in classes:
        os.makedirs(f"{new_path}/{split}/{cls}", exist_ok=True)

# Proses mengisi folder tiap kelas
for cls in classes:
    # ambil data dari base path
    cls_path = os.path.join(path, cls)
    images = [f for f in os.listdir(cls_path)
              if f.lower().endswith(('.png', '.jpg', '.jpeg'))]

    # mengambil gamabr random
    random.shuffle(images)

    # Menghitung jumlah setiap split
    total_images = len(images)
    train_count = int(total_images * train_ratio)
    val_count = int(total_images * val_ratio)
    test_count = total_images - train_count - val_count

    print(f"\nKelas {cls}: {total_images} gambar")
    print(f"  - Train: {train_count} gambar")
    print(f"  - Validation: {val_count} gambar")
    print(f"  - Test: {test_count} gambar")

    # Split data untuk train, val, test
    train_images = images[:train_count]
    val_images = images[train_count:train_count + val_count]
    test_images = images[train_count + val_count:]

    # Copy files random tadi ke folder yang sesuai
    for img in train_images:
        src = os.path.join(cls_path, img)
        dst = os.path.join(new_path, 'train', cls, img)
        shutil.copy2(src, dst)

    for img in val_images:
        src = os.path.join(cls_path, img)
        dst = os.path.join(new_path, 'val', cls, img)
        shutil.copy2(src, dst)

    for img in test_images:
        src = os.path.join(cls_path, img)
        dst = os.path.join(new_path, 'test', cls, img)
        shutil.copy2(src, dst)

# Verifikasi hasil copy data untuk split
for split in ['train', 'val', 'test']:
    print(f"\n{split.upper()}:")
    total_split = 0
    for cls in classes:
        count = len(os.listdir(f"{new_path}/{split}/{cls}"))
        print(f"  {cls}: {count} gambar")
        total_split += count
    print(f"  Total {split}: {total_split} gambar")

###
## Copy data Split dari Colab ke Drive
###
colab_data = '/content/satellite_dataset_split'

if os.path.exists(colab_data):
    # Cek isi data
    print("Isi data split:")
    for item in os.listdir(colab_data):
        item_path = os.path.join(colab_data, item)
        if os.path.isdir(item_path):
            count = len(os.listdir(item_path))
            print(f" file {item}/ - {count} subfolder")

    # Target lokasi untuk dipindah
    drive_path = '/content/drive/MyDrive/satellite_dataset_split'
    print(f"\nMemindahkan data ke Drive")

    # Copy data dari Colab ke Drive
    if not os.path.exists(drive_path):
        shutil.copytree(colab_data, drive_path)
        print("Data berhasil dipindah ke Google Drive!")
    else:
        print("Data sudah ada di Google Drive")

    # Verifikasi data sudah ada di Drive
    print(f"\n Lokasi data di Drive: {drive_path}")

    for item in os.listdir(drive_path):
        item_path = os.path.join(drive_path, item)
        if os.path.isdir(item_path):
            # Hitung jumlah gambar di setiap folder
            total_images = 0
            subfolders = os.listdir(item_path)
            print(f"  üìÅ {item}/")

            for subfolder in subfolders:
                subfolder_path = os.path.join(item_path, subfolder)
                if os.path.isdir(subfolder_path):
                    count = len([f for f in os.listdir(subfolder_path)
                               if f.lower().endswith(('.jpg', '.jpeg', '.png'))])
                    total_images += count
                    print(f"    üìÇ {subfolder}: {count} gambar")

            print(f"Total {item}: {total_images} gambar")

    print(f"Data sekarang tersimpan di Google Drive Anda")

else:
    print("Data split tidak ditemukan ")

"""## Modelling
Membangun model CNN
"""

###
## Setup Base Data Yang Digunakan
###

# Base Data menggunakan satellite_dataset_split
data_path = "/content/satellite_dataset_split"

# Setting random
tf.random.set_seed(42)

# Parameter gambar
IMG_SIZE = 224
BATCH_SIZE = 32
NUM_CLASSES = 4  # cloudy, desert, green_area, water

# Data generator untuk train
train_data = ImageDataGenerator(
    rescale=1./255,
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    horizontal_flip=True,
    zoom_range=0.2
)

# Data Generator untuk validation dan test
val_test_data = ImageDataGenerator(rescale=1./255)


###
## Load Data Training, Testing, Validation
###

# Load data training
train_generator = train_data.flow_from_directory(
    f"{data_path}/train",
    target_size=(IMG_SIZE, IMG_SIZE),
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    shuffle=True
)

# Load data validation
val_generator = val_test_data.flow_from_directory(
    f"{data_path}/val",
    target_size=(IMG_SIZE, IMG_SIZE),
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    shuffle=False
)

# Load data test
test_generator = val_test_data.flow_from_directory(
    f"{data_path}/test",
    target_size=(IMG_SIZE, IMG_SIZE),
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    shuffle=False
)

print("Kelas yang ditemukan:", train_generator.class_indices)

###
## Build Model CNN
###

model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 3)),  # Conv1
    MaxPooling2D(2, 2),

    Conv2D(64, (3, 3), activation='relu'),     # Conv2
    MaxPooling2D(2, 2),

    Conv2D(128, (3, 3), activation='relu'),    # Conv3
    MaxPooling2D(2, 2),

    Conv2D(256, (3, 3), activation='relu'),    # Conv4
    MaxPooling2D(2, 2),

    Flatten(),  # Mengubah dari 2D ke 1D, diratain gitu (flat)

    # Fully Connected Layers
    Dense(512, activation='relu'),
    Dropout(0.5),  # Prevent overfitting
    Dense(256, activation='relu'),
    Dropout(0.3),

    Dense(NUM_CLASSES, activation='softmax')  # Output layer
])


###
## Compile dan Menampilkan Model CNN
###
model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)
model.summary()

"""## Training Model"""

# Stop training jika model ditrain tidak improve
early_stopping = EarlyStopping(
    monitor='val_loss',
    patience=5,
    restore_best_weights=True
)

# Mengurangi learning rate jika stuck
reduce_lr = ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.2,
    patience=3,
    min_lr=0.0001
)

callbacks = [early_stopping, reduce_lr]

EPOCHS = 20

history = model.fit(
    train_generator,
    epochs=EPOCHS,
    validation_data=val_generator,
    callbacks=callbacks,
    verbose=1
)

"""## Evaluasi dan Visualisasi"""

# Menampilkan hasil akurasi model cnn
test_loss, test_accuracy = model.evaluate(test_generator, verbose=0)
print(f"Test Loss: {test_loss:.4f}")
print(f"Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)")

# Menampilkan plot grafik training
def plot_training_history(history):
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))

    # Plot accuracy
    ax1.plot(history.history['accuracy'], label='Training Accuracy')
    ax1.plot(history.history['val_accuracy'], label='Validation Accuracy')
    ax1.set_title('Model Accuracy')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Accuracy')
    ax1.legend()
    ax1.grid(True)

    # Plot loss
    ax2.plot(history.history['loss'], label='Training Loss')
    ax2.plot(history.history['val_loss'], label='Validation Loss')
    ax2.set_title('Model Loss')
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('Loss')
    ax2.legend()
    ax2.grid(True)

    plt.tight_layout()
    plt.show()

plot_training_history(history)

"""## Konversi Model"""

###
## SavedModel (.pb)
##

# Path untuk menyimpan SavedModel
savedmodel_dir = "/content/drive/MyDrive/saved_model"

# Menghapus folder lama jika ada
if os.path.exists(savedmodel_dir):
    shutil.rmtree(savedmodel_dir)

# Menyimpan model dalam format SavedModel
tf.saved_model.save(model, savedmodel_dir)

# Melakukan verifikasi struktur file SavedModel
print(f"\nStruktur file SavedModel di: {savedmodel_dir}")
for root, dirs, files in os.walk(savedmodel_dir):
    level = root.replace(savedmodel_dir, '').count(os.sep)
    indent = ' ' * 2 * level
    print(f"{indent}{os.path.basename(root)}/")
    subindent = ' ' * 2 * (level + 1)
    for file in files:
        file_size = os.path.getsize(os.path.join(root, file)) / (1024*1024)
        print(f"{subindent}{file} ({file_size:.2f} MB)")

# Mengetahui ukuran file savedmodel
total_size = sum(os.path.getsize(os.path.join(dirpath, filename))
                for dirpath, dirnames, filenames in os.walk(savedmodel_dir)
                for filename in filenames) / (1024*1024)
print(f"\nTotal ukuran SavedModel: {total_size:.2f} MB")

###
## TF-Lite Model
##

# Path untuk TFLite
tflite_dir = '/content/drive/MyDrive/tflite_model'
os.makedirs(tflite_dir, exist_ok=True)

# Menggunakan Converter untuk konversi ke TFLite
converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
tflite_model = converter.convert()

# Menyimpan file .tflite
tflite_path = os.path.join(tflite_dir, 'satellite_model.tflite')
with open(tflite_path, 'wb') as f:
    f.write(tflite_model)

# Menyimpan file label
class_labels = ['cloudy', 'desert', 'green_area', 'water']
label_path = os.path.join(tflite_dir, 'labels.txt')
with open(label_path, 'w') as f:
    for i, label in enumerate(class_labels):
        f.write(f"{i} {label}\n")

tflite_size = os.path.getsize(tflite_path) / (1024*1024)
print(f"‚úì TFLite tersimpan! Ukuran: {tflite_size:.2f} MB")
print(f"  File: {tflite_path}")
print(f"  Label: {label_path}")

###
## TFJS Model
##

# Path untuk TFJS
tfjs_dir = '/content/drive/MyDrive/tfjs_model'

if os.path.exists(tfjs_dir):
    shutil.rmtree(tfjs_dir)
os.makedirs(tfjs_dir, exist_ok=True)

# Konversi dan simpan ke TFJS
tfjs.converters.save_keras_model(model, tfjs_dir)

# Mengetahui ukuran file TFJS
tfjs_total_size = sum(os.path.getsize(os.path.join(tfjs_dir, f))
                     for f in os.listdir(tfjs_dir)
                     if os.path.isfile(os.path.join(tfjs_dir, f))) / (1024*1024)
print(f"‚úì TFJS tersimpan! Ukuran: {tfjs_total_size:.2f} MB")
print(f"  Lokasi: {tfjs_dir}")

# Menyimpan model .keras untuk inference
keras_path = '/content/drive/MyDrive/satelite.keras'
model.save(keras_path)
print(f"Model berhasil disimpan di: {keras_path}")

"""## Inference (Optional)"""

# Kelas label dan Path gambar untuk testing
class_labels = ['cloudy', 'desert', 'green_area', 'water']
img_dir1 = '/content/drive/MyDrive/Submission/Deep Learning/Image Classification/Inference/hurricane-63123_1280.jpg'
img_dir2 = '/content/drive/MyDrive/Submission/Deep Learning/Image Classification/Inference/cyclone-62957_1280.jpg'
img_dir3 = '/content/drive/MyDrive/Submission/Deep Learning/Image Classification/Inference/meteor-67495_1280.jpg'

###
## Inference menggunakan SavedModel
###

# Load SavedModel
savedmodel_path = "/content/drive/MyDrive/Submission/Deep Learning/Image Classification/saved_model"
loaded_savedmodel = tf.saved_model.load(savedmodel_path)

# Load dan preprocess gambar
original_img = Image.open(img_dir1)
print(f"Resolusi asli: {original_img.size[0]} x {original_img.size[1]} pixels")

img = image.load_img(img_dir1, target_size=(224, 224))
img_array = image.img_to_array(img) / 255.0
img_array = np.expand_dims(img_array, axis=0).astype(np.float32)
plt.imshow(img)
plt.axis('off')
plt.title("Satellite Image Capture")
plt.show()

# Prediksi dengan SavedModel
infer = loaded_savedmodel.signatures["serving_default"]
prediction_savedmodel = infer(tf.constant(img_array))

# Ambil hasil prediksi
try:
    pred_output = prediction_savedmodel['dense_3']
except:
    try:
        pred_output = prediction_savedmodel['output_0']
    except:
        pred_output = list(prediction_savedmodel.values())[0]

pred_savedmodel = pred_output.numpy()
predicted_idx = np.argmax(pred_savedmodel)
confidence = pred_savedmodel[0][predicted_idx] * 100

print(f"SavedModel Prediksi: {class_labels[predicted_idx]} ({confidence:.1f}%)")

###
## Inference menggunakan SavedModel
###

# Load SavedModel
savedmodel_path = "/content/drive/MyDrive/Submission/Deep Learning/Image Classification/saved_model"
loaded_savedmodel = tf.saved_model.load(savedmodel_path)

# Load dan preprocess gambar
original_img = Image.open(img_dir2)
print(f"Resolusi asli: {original_img.size[0]} x {original_img.size[1]} pixels")

img = image.load_img(img_dir2, target_size=(224, 224))
img_array = image.img_to_array(img) / 255.0
img_array = np.expand_dims(img_array, axis=0).astype(np.float32)
plt.imshow(img)
plt.axis('off')
plt.title("Satellite Image Capture")
plt.show()

# Prediksi dengan SavedModel
infer = loaded_savedmodel.signatures["serving_default"]
prediction_savedmodel = infer(tf.constant(img_array))

# Ambil hasil prediksi
try:
    pred_output = prediction_savedmodel['dense_3']
except:
    try:
        pred_output = prediction_savedmodel['output_0']
    except:
        pred_output = list(prediction_savedmodel.values())[0]

pred_savedmodel = pred_output.numpy()
predicted_idx = np.argmax(pred_savedmodel)
confidence = pred_savedmodel[0][predicted_idx] * 100

print(f"SavedModel Prediksi: {class_labels[predicted_idx]} ({confidence:.1f}%)")

###
## Inference menggunakan SavedModel
###

# Load SavedModel
savedmodel_path = "/content/drive/MyDrive/Submission/Deep Learning/Image Classification/saved_model"
loaded_savedmodel = tf.saved_model.load(savedmodel_path)

# Load dan preprocess gambar
original_img = Image.open(img_dir3)
print(f"Resolusi asli: {original_img.size[0]} x {original_img.size[1]} pixels")

img = image.load_img(img_dir3, target_size=(224, 224))
img_array = image.img_to_array(img) / 255.0
img_array = np.expand_dims(img_array, axis=0).astype(np.float32)
plt.imshow(img)
plt.axis('off')
plt.title("Satellite Image Capture")
plt.show()

# Prediksi dengan SavedModel
infer = loaded_savedmodel.signatures["serving_default"]
prediction_savedmodel = infer(tf.constant(img_array))

# Ambil hasil prediksi
try:
    pred_output = prediction_savedmodel['dense_3']
except:
    try:
        pred_output = prediction_savedmodel['output_0']
    except:
        pred_output = list(prediction_savedmodel.values())[0]

pred_savedmodel = pred_output.numpy()
predicted_idx = np.argmax(pred_savedmodel)
confidence = pred_savedmodel[0][predicted_idx] * 100

print(f"SavedModel Prediksi: {class_labels[predicted_idx]} ({confidence:.1f}%)")